{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.gat import GATNet\n",
    "from models.gat_gcn import GAT_GCN\n",
    "from models.gcn import GCNNet\n",
    "from models.ginconv import GINConvNet\n",
    "from utils import *\n",
    "\n",
    "# training function at each epoch\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    print('Training on {} samples...'.format(len(train_loader.dataset)))\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, data.y.view(-1, 1).float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
    "                                                                           batch_idx * len(data.x),\n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader),\n",
    "                                                                           loss.item()))\n",
    "\n",
    "def predicting(model, device, loader):\n",
    "    model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "    print('Make predic(tion for {} samples...'.format(len(loader.dataset)))\n",
    "    with torch.no_grad):\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
    "            total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
    "    return total_labels.numpy().flatten(),total_preds.numpy().flatten()\n",
    "\n",
    "\n",
    "datasets = [['davis','kiba'][int(sys.argv[1])]] \n",
    "modeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][int(sys.argv[2])]\n",
    "model_st = modeling.__name__\n",
    "\n",
    "cuda_name = \"cuda:0\"\n",
    "if len(sys.argv)>3:\n",
    "    cuda_name = [\"cuda:0\",\"cuda:1\"][int(sys.argv[3])]\n",
    "print('cuda_name:', cuda_name)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 512\n",
    "TEST_BATCH_SIZE = 512\n",
    "LR = 0.0005\n",
    "LOG_INTERVAL = 20\n",
    "NUM_EPOCHS = 100                                #这里不一样\n",
    "\n",
    "print('Learning rate: ', LR)\n",
    "print('Epochs: ', NUM_EPOCHS)\n",
    "\n",
    "# Main program: iterate over different datasets\n",
    "for dataset in datasets:\n",
    "    print('\\nrunning on ', model_st + '_' + dataset )\n",
    "    processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n",
    "    processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n",
    "    if ((not os.path.isfile(processed_data_file_train)) or (not os.path.isfile(processed_data_file_test))):\n",
    "        print('please run create_data.py to prepare data in pytorch format!')\n",
    "    else:\n",
    "        train_data = TestbedDataset(root='data', dataset=dataset+'_train')\n",
    "        test_data = TestbedDataset(root='data', dataset=dataset+'_test')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "#到这里上边的代码和training.py里的代码都是一样的，都是创建了训练和预测函数，加载了数据集，下来不一样在于这里将训练集分为80%和20%的验证集，用于和DeepDTA模型作对比\n",
    "        train_size = int(0.8 * len(train_data))                                                       #得到80%的数据量为多少\n",
    "        valid_size = len(train_data) - train_size                                                     #剩下的都为验证的数据集\n",
    "        train_data, valid_data = torch.utils.data.random_split(train_data, [train_size, valid_size])  #按照8:2随机划分训练和验证集\n",
    "\n",
    "        # make data PyTorch mini-batch processing ready\n",
    "        train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "        test_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # training the model\n",
    "        device = torch.device(cuda_name if torch.cuda.is_available() else \"cpu\")\n",
    "        model = modeling().to(device)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "        best_mse = 1000\n",
    "        best_test_mse = 1000\n",
    "        best_test_ci = 0\n",
    "        best_epoch = -1\n",
    "        model_file_name = 'model_' + model_st + '_' + dataset +  '.model'\n",
    "        result_file_name = 'result_' + model_st + '_' + dataset +  '.csv'\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            train(model, device, train_loader, optimizer, epoch+1)\n",
    "            print('predicting for valid data')\n",
    "            G,P = predicting(model, device, valid_loader)\n",
    "            val = mse(G,P)                                                         #通过验证集保留验证集的mse来迭代，依据是验证集上的mse，在未知数据集上验证，提高模型鲁棒性\n",
    "            if val<best_mse:\n",
    "                best_mse = val\n",
    "                best_epoch = epoch+1\n",
    "                torch.save(model.state_dict(), model_file_name)\n",
    "                print('predicting for test data')                                  #预测测试集上的数据，并将其五个指标保存下来\n",
    "                G,P = predicting(model, device, test_loader)\n",
    "                ret = [rmse(G,P),mse(G,P),pearson(G,P),spearman(G,P),ci(G,P)]\n",
    "                with open(result_file_name,'w') as f:\n",
    "                    f.write(','.join(map(str,ret)))\n",
    "                best_test_mse = ret[1]\n",
    "                best_test_ci = ret[-1]\n",
    "                print('rmse improved at epoch ', best_epoch, '; best_test_mse,best_test_ci:', best_test_mse,best_test_ci,model_st,dataset)\n",
    "            else:\n",
    "                print(ret[1],'No improvement since epoch ', best_epoch, '; best_test_mse,best_test_ci:', best_test_mse,best_test_ci,model_st,dataset)                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
